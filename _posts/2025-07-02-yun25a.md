---
title: 'Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?'
abstract: 'Medical research faces well-documented challenges in translating novel
  treatments into clinical practice. Publishing incentives encourage researchers to
  present "positive" findings, even when empirical results are equivocal. Consequently,
  it is well-documented that authors often spin study results, especially in article
  abstracts. Such spin can influence clinician interpretation of evidence and may
  affect patient care decisions. In this study, we ask whether the interpretation
  of trial results offered by Large Language Models (LLMs) is similarly affected by
  spin. This is important since LLMs are increasingly being used to trawl through
  and synthesize published medical evidence. We evaluated 22 LLMs and found that they
  are across the board more susceptible to spin than humans. They might also propagate
  spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate
  spin into plain language summaries that they generate. We also find, however, that
  LLMs are generally capable of recognizing spin, and can be prompted in a way to
  mitigate spinâ€™s impact on LLM outputs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yun25a
month: 0
tex_title: 'Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?'
firstpage: 458
lastpage: 479
page: 458-479
order: 458
cycles: false
bibtex_author: Yun, Hye Sun and Zhang, Karen Y.C. and Kouzy, Ramez and Marshall, Iain
  James and Li, Junyi Jessy and Wallace, Byron C
author:
- given: Hye Sun
  family: Yun
- given: Karen Y.C.
  family: Zhang
- given: Ramez
  family: Kouzy
- given: Iain James
  family: Marshall
- given: Junyi Jessy
  family: Li
- given: Byron C
  family: Wallace
date: 2025-07-02
address:
container-title: Proceedings of the sixth Conference on Health, Inference, and Learning
volume: '287'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v287/main/assets/yun25a/yun25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
