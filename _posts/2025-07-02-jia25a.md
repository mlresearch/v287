---
title: How does my language model understand clinical text?
abstract: 'Large language models (LLMs) have performed well across various tasks in
  clinical natural language processing tasks, despite not being directly trained on
  electronic health record (EHR) data. In this work, we examine how popular open-source
  LLMs learn clinical information from large mined corpora through two crucial but
  understudied lenses: (1) their interpretation of clinical jargon, a foundational
  ability for understanding real-world clinical notes, and (2) their responses to
  medical misinformation. For both use cases, we investigate the frequency of relevant
  clinical information in their corresponding pretraining corpora, the relationship
  between pretraining data composition and model outputs, and the sources underlying
  this data. To isolate clinical jargon understanding, we evaluate LLMs on a new dataset
  MedLingo. Unsurprisingly, we find that the frequency of clinical jargon mentions
  across major pretraining corpora correlates with model performance. However, jargon
  frequently appearing in clinical notes often rarely appears in pretraining corpora,
  revealing a mismatch between available data and real-world usage. Similarly, we
  find that a non-negligible portion of documents support disputed claims that can
  then be parroted by models. Finally, we classified and analyzed the types of online
  sources in which clinical jargon and misinformation appear, with implications for
  future dataset composition.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jia25a
month: 0
tex_title: How does my language model understand clinical text?
firstpage: 720
lastpage: 743
page: 720-743
order: 720
cycles: false
bibtex_author: Jia, Furong and Sontag, David and Agrawal, Monica
author:
- given: Furong
  family: Jia
- given: David
  family: Sontag
- given: Monica
  family: Agrawal
date: 2025-07-02
address:
container-title: Proceedings of the sixth Conference on Health, Inference, and Learning
volume: '287'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v287/main/assets/jia25a/jia25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
